{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwxxxqqY_XiY"
      },
      "outputs": [],
      "source": [
        "# ✅ Install Required Libraries\n",
        "!pip install kagglehub torch torchvision matplotlib scikit-learn\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
        "import kagglehub\n",
        "\n",
        "#  Download Malimg Dataset using Kaggle Hub\n",
        "dataset_path = kagglehub.dataset_download(\"manmandes/malimg\")\n",
        "dataset_path = os.path.join(dataset_path, \"malimg_dataset\", \"train\")\n",
        "print(f\"Dataset loaded from: {dataset_path}\")\n",
        "\n",
        "# ✅ Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ✅ Define Data Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),  # Resize to speed up training\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)  # Normalize\n",
        "])\n",
        "\n",
        "# ✅ Load Dataset\n",
        "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# ✅ Define Deep Autoencoder\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded\n",
        "\n",
        "# ✅ Initialize Model\n",
        "model = Autoencoder().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# ✅ Train Autoencoder\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for images, _ in dataloader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward Pass\n",
        "        encoded, decoded = model(images)\n",
        "        loss = criterion(decoded, images)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}\")\n",
        "\n",
        "# ✅ Extract Features for Clustering\n",
        "model.eval()\n",
        "latent_features = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, lbls in dataloader:\n",
        "        images = images.to(device)\n",
        "        encoded, _ = model(images)\n",
        "        latent_features.append(encoded.view(images.size(0), -1).cpu().numpy())\n",
        "        labels.extend(lbls.numpy())\n",
        "\n",
        "latent_features = np.vstack(latent_features)\n",
        "\n",
        "# ✅ Apply K-Means Clustering\n",
        "num_clusters = len(dataset.classes)\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(latent_features)\n",
        "\n",
        "# ✅ Evaluate Clustering\n",
        "ari = adjusted_rand_score(labels, clusters)\n",
        "silhouette = silhouette_score(latent_features, clusters)\n",
        "\n",
        "print(f\"ARI Score: {ari:.4f}\")\n",
        "print(f\"Silhouette Score: {silhouette:.4f}\")\n",
        "\n",
        "# ✅ Visualize Clusters\n",
        "pca = PCA(n_components=2)\n",
        "latent_2d = pca.fit_transform(latent_features)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=clusters, cmap=\"viridis\", alpha=0.6)\n",
        "plt.title(\"Malimg Dataset Clustering with DAC\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.colorbar(label=\"Cluster\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "#import kagglehub\n",
        "dataset_path = kagglehub.dataset_download(\"manmandes/malimg\")\n",
        "train_path = os.path.join(dataset_path, \"malimg_dataset\", \"train\")\n",
        "test_path = os.path.join(dataset_path, \"malimg_dataset\", \"test\")\n",
        "\n",
        "print(f\"Dataset loaded from: {dataset_path}\")\n",
        "\n",
        "\n",
        "# Step 1: Download Malimg Dataset using Kaggle Hub\n",
        "dataset_path = kagglehub.dataset_download(\"manmandes/malimg\")\n",
        "\n",
        "# Print the path where the dataset is downloaded\n",
        "print(f\"Dataset downloaded to: {dataset_path}\")\n",
        "\n",
        "# Adjust the path based on the downloaded directory structure\n",
        "dataset_path = os.path.join(dataset_path, \"malimg_dataset\", \"train\")\n",
        "print(f\"Dataset loaded from: {dataset_path}\")\n",
        "\n",
        "\n",
        "class MalimgDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = [\n",
        "            os.path.join(root_dir, img)\n",
        "            for img in os.listdir(root_dir)\n",
        "            if os.path.isfile(os.path.join(root_dir, img))  # Only include files\n",
        "        ]\n",
        "\n",
        "        # Debug: Check how many images were loaded\n",
        "        print(f\"Loaded {len(self.image_paths)} images from {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "# ---------------------- STEP 2: Define MalimgDataset Class ----------------------\n",
        "# class MalimgDataset(Dataset):\n",
        "#     def __init__(self, root_dir, transform=None):\n",
        "#         self.root_dir = root_dir\n",
        "#         self.transform = transform\n",
        "\n",
        "#         self.image_paths = [\n",
        "#             os.path.join(root_dir, img)\n",
        "#             for img in os.listdir(root_dir)\n",
        "#             if os.path.isfile(os.path.join(root_dir, img))  # Only include files\n",
        "#         ]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.image_paths)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_path = self.image_paths[idx]\n",
        "#         image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "#         if self.transform:\n",
        "#             image = self.transform(image)\n",
        "\n",
        "#         return image\n",
        "\n",
        "\n",
        "\n",
        "# print(f\"Number of batches processed: {len(features_list)}\")\n",
        "# print(f\"Number of images in dataset: {len(dataset)}\")\n",
        "\n",
        "# Load Dataset\n",
        "dataset = MalimgDataset(root_dir=dataset_path, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Debug: Check number of batches and images\n",
        "print(f\"Number of batches: {len(dataloader)}\")\n",
        "print(f\"Number of images in dataset: {len(dataset)}\")\n",
        "\n",
        "# Check a few sample images to ensure they are being loaded\n",
        "for i, images in enumerate(dataloader):\n",
        "    print(f\"Batch {i + 1} - Number of images: {len(images)}\")\n",
        "    if i == 1:  # Show only a few batches for debugging\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "# # Image Transformations\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "\n",
        "# # Load Dataset\n",
        "# dataset = MalimgDataset(root_dir=dataset_path, transform=transform)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Now you can continue with the rest of your code...\n",
        "\n",
        "# ---------------------- STEP 1: Load Malimg Dataset & Extract Features from CNN ----------------------\n",
        "\n",
        "# class MalimgDataset(Dataset):\n",
        "#     def __init__(self, root_dir, transform=None):\n",
        "#         self.root_dir = root_dir\n",
        "#         self.transform = transform\n",
        "#         self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir)]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.image_paths)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         img_path = self.image_paths[idx]\n",
        "#         image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "#         if self.transform:\n",
        "#             image = self.transform(image)\n",
        "\n",
        "#         return image\n",
        "\n",
        "# # Image Transformations\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "# ])\n",
        "\n",
        "# # Load Dataset\n",
        "# dataset_path = \"malimg_dataset/train\"  # Adjust the path based on your dataset\n",
        "# dataset = MalimgDataset(root_dir=dataset_path, transform=transform)\n",
        "# dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# Load Pretrained CNN (ResNet as Example)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.resnet18(pretrained=True)  # Can replace with GoogleNet, Xception, etc.\n",
        "model.fc = nn.Identity()  # Remove final classification layer\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Extract Features from Penultimate Layer\n",
        "features_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images in tqdm(dataloader, desc=\"Extracting Features\"):\n",
        "        images = images.to(device)\n",
        "        features = model(images)\n",
        "        features_list.append(features.cpu().numpy())\n",
        "\n",
        "features_array = np.vstack(features_list)\n",
        "\n",
        "print(f\"Feature extraction completed! Shape: {features_array.shape}\")  # (Num_samples, Feature_dim)\n",
        "\n",
        "# ---------------------- STEP 2: Train Deep Autoencoder (DAC) ----------------------\n",
        "\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, input_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return encoded, decoded\n",
        "\n",
        "# Initialize Autoencoder\n",
        "input_dim = features_array.shape[1]\n",
        "autoencoder = Autoencoder(input_dim).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
        "\n",
        "# Convert Features to Torch Tensor\n",
        "features_tensor = torch.tensor(features_array, dtype=torch.float32).to(device)\n",
        "\n",
        "# Train Autoencoder\n",
        "num_epochs = 50\n",
        "batch_size = 32\n",
        "dataset_size = len(features_tensor)\n",
        "\n",
        "print(\"Training Deep Autoencoder...\")\n",
        "for epoch in range(num_epochs):\n",
        "    perm = torch.randperm(dataset_size)\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i in range(0, dataset_size, batch_size):\n",
        "        batch_idx = perm[i:i+batch_size]\n",
        "        batch = features_tensor[batch_idx]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        encoded, decoded = autoencoder(batch)\n",
        "        loss = criterion(decoded, batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "print(\"Autoencoder training completed!\")\n",
        "\n",
        "# Extract Compressed Features from Encoder\n",
        "with torch.no_grad():\n",
        "    compressed_features = autoencoder.encoder(features_tensor).cpu().numpy()\n",
        "\n",
        "print(f\"Compressed feature shape: {compressed_features.shape}\")  # (Num_samples, 128)\n",
        "\n",
        "# ---------------------- STEP 3: Apply K-Means & DAC Clustering ----------------------\n",
        "\n",
        "# Apply K-Means on Raw CNN Features\n",
        "kmeans_raw = KMeans(n_clusters=10, random_state=42)\n",
        "kmeans_raw_labels = kmeans_raw.fit_predict(features_array)\n",
        "\n",
        "# Apply K-Means on Compressed Autoencoder Features (DAC)\n",
        "kmeans_dac = KMeans(n_clusters=10, random_state=42)\n",
        "kmeans_dac_labels = kmeans_dac.fit_predict(compressed_features)\n",
        "\n",
        "# ---------------------- STEP 4: Visualize Clustering with t-SNE ----------------------\n",
        "\n",
        "def plot_clusters(data, labels, title):\n",
        "    tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
        "    reduced_data = tsne.fit_transform(data)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
        "    plt.colorbar()\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Plot Raw CNN Features Clustering\n",
        "plot_clusters(features_array, kmeans_raw_labels, \"K-Means Clustering on Raw CNN Features\")\n",
        "\n",
        "# Plot DAC Clustering\n",
        "plot_clusters(compressed_features, kmeans_dac_labels, \"Deep Autoencoder-Based Clustering (DAC)\")\n",
        "\n",
        "print(\"Clustering & Visualization Completed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "SH2mfT29X7Xv",
        "outputId": "bcee8477-ba44-419f-b1d4-a045a59434a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded from: /root/.cache/kagglehub/datasets/manmandes/malimg/versions/1\n",
            "Dataset downloaded to: /root/.cache/kagglehub/datasets/manmandes/malimg/versions/1\n",
            "Dataset loaded from: /root/.cache/kagglehub/datasets/manmandes/malimg/versions/1/malimg_dataset/train\n",
            "Loaded 0 images from /root/.cache/kagglehub/datasets/manmandes/malimg/versions/1/malimg_dataset/train\n",
            "Number of batches: 0\n",
            "Number of images in dataset: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "need at least one array to concatenate",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-305e489c4308>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mfeatures_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m \u001b[0mfeatures_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_list\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Feature extraction completed! Shape: {features_array.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (Num_samples, Feature_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/_core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
          ]
        }
      ]
    }
  ]
}